\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{colortbl} % For colored table cells
\usepackage{booktabs} % For better tables
\usepackage{multirow} % For multirow cells in tables
\usepackage{url}      % For URLs
\usepackage{hyperref} % For clickable links (optional)

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Define colors for comments or notes (optional)
\usepackage{xcolor}
\newcommand{\todo}[1]{\textcolor{purple}{[TODO: #1]}}
\newcommand{\note}[1]{\textcolor{teal}{[Note: #1]}}


\begin{document}

% \title{Scaling Up Cross-Encoder Reranking with 8K Token Inputs: A Comparative Study of Lion and AdamW on TREC DL 2019} % 
\title{Optimal Design of Cross-Encoder Reranking Models for Deep Retrieval}

% --- Author Information ---
% Use \and to separate authors if more than one
\author{\IEEEauthorblockN{Shahil Kumar} % Replace with author names
\IEEEauthorblockA{\textit{IT} \\ % Replace with affiliation
\textit{IIIT Allahabad}\\
Prayagraj, India \\
abc@email.com} % Replace with email
\and % Uncomment and repeat for additional authors
\IEEEauthorblockN{Author 2 Name}
\IEEEauthorblockA{\textit{Department} \\
\textit{Institution}\\
City, Country \\
email address}
}
% --- End Author Information ---

\maketitle

\begin{abstract}
Modern information retrieval systems often employ a two-stage pipeline consisting of an efficient initial retrieval stage followed by a more computationally intensive reranking stage. Cross-encoder models have demonstrated state-of-the-art effectiveness for the reranking task due to their ability to perform deep, contextualized analysis of query-document pairs. The choice of optimizer during the fine-tuning phase can significantly impact the final performance and training efficiency of these models. This paper investigates the impact of using the recently proposed Lion optimizer compared to the widely used AdamW optimizer for fine-tuning cross-encoder rerankers. We fine-tune three popular transformer models, `microsoft/MiniLM-L12-H384-uncased`, `Alibaba-NLP/gte-multilingual-base` and `answerdotai/ModernBERT-base`, on the MS MARCO passage ranking dataset using both optimizers. The effectiveness of the resulting models is evaluated on the TREC 2019 Deep Learning Track and MS-MARCO passage reranking task. Our experiments, facilitated by the Modal cloud computing platform for GPU offloading, show comparative results across different epochs. We analyze the performance trends based on standard IR metrics like NDCG@10, MAP, MRR@10 etc., providing insights into the effectiveness of Lion versus AdamW optimizer in this specific reranking context.
\end{abstract}

\begin{IEEEkeywords}
Information Retrieval, Cross-Encoder, Reranking, Optimizer, Lion Optimizer, AdamW, TREC Deep Learning, MS MARCO, Sentence Transformers, Modal.
\end{IEEEkeywords}

\section{Introduction}
Information Retrieval (IR) systems aim to satisfy a user's information need, often expressed as a textual query, by returning a ranked list of relevant documents from a large collection. A common and effective architecture for modern search systems is the two-stage retrieve-and-rerank pipeline \cite{Lin2021PretrainedTF}. The first stage employs computationally efficient methods, such as BM25 \cite{robertson2009probabilistic} or dense vector retrieval \cite{karpukhin2020dense}, to quickly retrieve a candidate set of documents (typically hundreds or thousands) from the entire collection. The second stage then applies a more sophisticated and computationally expensive reranking model to reorder this smaller candidate set, aiming for higher precision at the top ranks.

Cross-encoder models, typically based on transformer architectures like BERT \cite{devlin2019bert}, have emerged as state-of-the-art rerankers \cite{nogueira2019passage, Nogueira2020Document}. Unlike bi-encoder models that encode queries and documents independently, cross-encoders process the query and a candidate document simultaneously (e.g., `[CLS] query [SEP] document [SEP]`). This allows for deep, token-level interaction modeling, leading to superior relevance estimation accuracy, albeit at a higher computational cost suitable only for the second stage.

Fine-tuning these large transformer models effectively is crucial. The choice of optimizer plays a key role in navigating the complex loss landscape and achieving optimal performance. Adam \cite{kingma2014adam} and its variant AdamW \cite{loshchilov2019decoupled} are widely used and generally effective optimizers for training deep neural networks. Recently, the Lion (Evo\textbf{L}ved S\textbf{i}gn M\textbf{o}me\textbf{n}tum) optimizer \cite{chen2023symbolic} was proposed, derived through symbolic mathematics and program search. It claims improved performance and memory efficiency compared to AdamW on various tasks, particularly image classification and vision-language models.

In this work, we investigate the applicability and effectiveness of the Lion optimizer for fine-tuning cross-encoder models specifically for the task of passage reranking in information retrieval. We compare its performance against the standard AdamW optimizer. Our contributions are:
\begin{itemize}
    \item We fine-tune three distinct transformer-based cross-encoder models (`microsoft/MiniLM-L12-H384-uncased`, `Alibaba-NLP/gte-multilingual-base` and `answerdotai/ModernBERT-base`) on the large-scale MS MARCO passage dataset \cite{bajaj2016ms} using both Lion and AdamW optimizers.
    \item We evaluate the performance of the fine-tuned models across three training epochs on the TREC 2019 Deep Learning (DL) Track\cite{craswell2020overview} and MS-MARCO \cite{DBLP:journals/corr/NguyenRSGTMD16} passage reranking benchmark.
    \item We provide a comparative analysis of the optimizers' impact on reranking effectiveness using standard IR metrics (NDCG@10, MAP, MRR@10, Recall@10, R-Prec, P@10).
    \item We utilize the Modal cloud platform \cite{modal_labs} for efficient GPU resource management and reproducible experimentation.
\end{itemize}

The rest of the paper is organized as follows: Section \ref{sec:related_work} discusses related work. Section \ref{sec:methodology} details the models, optimizers, and training approach. Section \ref{sec:experimental_setup} describes the experimental setup, datasets, and evaluation protocol. Section \ref{sec:results} presents and discusses the results. Finally, Section \ref{sec:conclusion} concludes the paper and suggests future work.

\section{Related Work}
\label{sec:related_work}

\subsection{Cross-Encoder Reranking} 
The use of BERT-based models for document reranking was popularized by Nogueira et al. \cite{nogueira2019passage, Nogueira2020Document}. They demonstrated that fine-tuning BERT as a cross-encoder on relevance labels (like those from MS MARCO) significantly outperforms traditional IR models and even bi-encoder approaches for reranking tasks. Subsequent work explored various transformer architectures \cite{Lin2021PretrainedTF}, training strategies \cite{gao2021complementing}, and efficiency improvements \cite{hofstatter2020improving}. Models like MiniLM \cite{wang2020minilm} offer a balance between effectiveness and efficiency, while multilingual models like GTE \cite{li2023towards} (General Text Embeddings) provide strong performance across various text tasks, including retrieval, and can be adapted for cross-encoding. The `sentence-transformers` library \cite{reimers2019sentence} provides a convenient framework for training and using both bi-encoders and cross-encoders.

\subsection{Optimizers for Deep Learning}
Stochastic Gradient Descent (SGD) with momentum remains a fundamental optimizer, but adaptive learning rate methods like AdaGrad \cite{duchi2011adaptive}, RMSprop \cite{tieleman2012lecture}, and Adam \cite{kingma2014adam} often lead to faster convergence in practice for deep learning models. Adam combines momentum with adaptive scaling of learning rates based on estimates of first and second moments of the gradients. AdamW \cite{loshchilov2019decoupled} improves upon Adam by decoupling the weight decay regularization from the adaptive learning rate mechanism, often leading to better generalization. The Lion optimizer \cite{chen2023symbolic} takes a different approach, using only momentum tracking and a sign operation on the update, resulting in a simpler update rule (`update = sign(momentum) * lr`) and potentially requiring less memory due to not storing second moment estimates. Its effectiveness relative to AdamW has been shown primarily in vision and vision-language tasks, motivating its evaluation in the NLP/IR domain.

\subsection{Evaluation Benchmarks}
The MS MARCO (Microsoft MAchine Reading COmprehension) dataset \cite{bajaj2016ms} has become a standard benchmark for training and evaluating deep learning models for passage retrieval and reranking. It contains real user queries from Bing and human-judged relevant passages. The TREC Deep Learning (DL) Tracks \cite{craswell2020overview, craswell2021overview} provide challenging test collections built upon MS MARCO, using queries with sparse relevance judgments derived from pooling top results from various participating systems. We use the TREC DL 2019 passage ranking dataset, a standard benchmark for evaluating reranking effectiveness. Evaluation is typically performed using tools like `trec\_eval` \cite{trec_eval_github}, which calculates various standard IR metrics.

\section{Methodology}
\label{sec:methodology}

\subsection{Cross-Encoder Architecture} 
A cross-encoder model takes a query $q$ and a document $d$ as input, typically concatenating them with special tokens: `[CLS] q [SEP] d [SEP]`. This combined sequence is fed into a transformer model (e.g., BERT, MiniLM, GTE). The output representation corresponding to the `[CLS]` token is then passed through a linear layer followed by a sigmoid activation (for binary classification) or used directly (for regression) to predict a relevance score $s(q, d)$. During inference for reranking, this score is computed for all candidate documents retrieved in the first stage, and the documents are re-sorted based on these scores.

\subsection{Base Models}
We experiment with three transformer base models:
\begin{itemize}
    \item \textbf{`microsoft/MiniLM-L12-H384-uncased` \cite{wang2020minilm}:} A distilled version of BERT, designed to be smaller and faster while retaining significant performance. It uses knowledge distillation from a larger teacher model during pre-training. It has 12 layers and a hidden size of 384.
    \item \textbf{`Alibaba-NLP/gte-multilingual-base` \cite{li2023towards}:} Part of the General Text Embeddings (GTE) family, trained on a large, diverse corpus covering multiple domains and languages. While often used as a bi-encoder for text embeddings, its underlying transformer architecture can be effectively fine-tuned as a cross-encoder. This is the 'base' sized version.
    \item \textbf{`answerdotai/ModernBERT-base` \cite{modernbert}:} ModernBERT is a state-of-the-art encoder-only transformer model that introduces several architectural enhancements over the original BERT. These include Rotary Positional Embeddings\cite{su2023roformerenhancedtransformerrotary} for extended context lengths, Flash Attention\cite{dao2022flashattentionfastmemoryefficientexact} for faster processing, GeGLU activation functions\cite{shazeer2020gluvariantsimprovetransformer}, and a combination of local and global attention mechanisms. Trained on 2 trillion tokens, ModernBERT achieves superior performance across various classification and retrieval tasks while maintaining efficiency suitable for deployment on common GPUs.
\end{itemize}

\subsection{Training}
We fine-tune the cross-encoders using the MS MARCO passage ranking triplets dataset, as processed by `sentence-transformers` \cite{reimers2019sentence}. The original dataset contains tuples of (query, positive passage, negative passage). We convert this into pairs `(query, positive\_passage)` with label 1 and `(query, negative\_passage)` with label 0.
The training objective is to minimize the Binary Cross-Entropy (BCE) loss between the predicted relevance score (output of the sigmoid function) and the true label (0 or 1). The BCE loss is defined as:
$$ \mathcal{L}_{BCE} = - [y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})] $$
where $y$ is the true label and $\hat{y}$ is the model's predicted probability.

\subsection{Optimizers}
We compare two optimizers:
\begin{itemize}
    \item \textbf{AdamW \cite{loshchilov2019decoupled}:} A widely adopted optimizer that modifies the original Adam algorithm to decouple weight decay from the gradient update step, often leading to improved regularization and generalization.
    \item \textbf{Lion \cite{chen2023symbolic}:} Discovered through automated symbolic search, Lion maintains only momentum and uses the sign of the momentum for the update direction. Its update rule is simpler than AdamW and does not require second moment estimates, potentially making it more memory-efficient. The update steps are as follows:

\begin{align*}
m_t &\leftarrow \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
\theta_t &\leftarrow \theta_{t-1} - \eta \cdot \text{sign}(\beta_2 m_t + (1 - \beta_2) g_t)
\end{align*}

\noindent where $g_t$ is the gradient, $\eta$ is the learning rate, and $\beta_1, \beta_2$ are momentum coefficients. The paper suggests default values of $\beta_1 = 0.9$ and $\beta_2 = 0.99$, though implementations may vary slightly. In our experiments, we use the `lion-pytorch`\cite{chen2023symbolic} implementation, which aligns with the paper's description.

\end{itemize}

\section{Experimental Setup}
\label{sec:experimental_setup}

\subsection{Datasets}
\begin{itemize}
    \item \textbf{Training:} We use the MS MARCO passage ranking triplets dataset \cite{bajaj2016ms}, processed into query-passage pairs with binary labels. We utilize the data loading strategy from `sentence-transformers`, sampling approximately 2 million training pairs derived from 1 million triplets, as specified in the `trainer.py` script. A small subset (10,000 pairs) is held out for validation during training, primarily for checkpoint selection based on loss, although final evaluation uses the TREC DL benchmark.
    \item \textbf{Evaluation:} We evaluate on the TREC 2019 Deep Learning Track passage ranking task \cite{craswell2020overview}. This dataset contains 43 queries with graded relevance judgments (qrels) for passages from the MS MARCO corpus.
\end{itemize}

\subsection{Implementation Details}
\begin{itemize}
    \item \textbf{Framework:} We use the `sentence-transformers` library \cite{reimers2019sentence} (specifically `CrossEncoder` and `CrossEncoderTrainer`) built on top of PyTorch and Hugging Face Transformers \cite{wolf2020transformers}.
    \item \textbf{Hyperparameters:} The key training parameters include:
        \begin{itemize}
            \item Batch Size: 512 (per device)
            \item Learning Rate: 2e-6 for Lion; AdamW typically uses  2e-5
            \item Optimizer Betas (Lion): (0.9, 0.99)
            \item Warmup Ratio: 0.1
            \item Epochs: 3
            \item Precision: BF16 is enabled (`bf16=True`).
            \item Seed: 12 (for reproducibility).
        \end{itemize}
    \item \textbf{Infrastructure:} Experiments were conducted using the Modal platform \cite{modal_labs}, leveraging NVIDIA A100-80GB GPUs for training and evaluation. Modal facilitated environment management (CUDA 12.4, Python 3.11, required libraries) and offloading computation to the cloud. Code scripts (`modal\_offload.py`, `trec\_modal.py`, `trainer.py`, `trec\_dl\_19\_eval\_2.py`) define the Modal functions and dependencies.
    \item \textbf{First-Stage Retrieval:} For the TREC DL evaluation, we first retrieve the top 1000 candidate passages per query using a standard BM25 baseline implemented with Pyserini \cite{lin2021pyserini} on the `msmarco-v1-passage` pre-built index. This constitutes the candidate set for reranking.
\end{itemize}

\subsection{Evaluation Protocol}
\begin{itemize}
    \item \textbf{Reranking:} Each fine-tuned cross-encoder model is used to score the top-1000 BM25 candidates for each TREC DL 2019 query. The passages are then reranked based on these cross-encoder scores.
    \item \textbf{Metrics:} We use the official `trec\_eval`\cite{trec_eval_github} tool to compute effectiveness metrics against the official TREC DL 2019 qrels. We report the following key metrics (though the provided JSON files contain many more):
        \begin{itemize}
            \item \textbf{NDCG@10} (Normalized Discounted Cumulative Gain at cutoff 10): Measures graded relevance and ranking quality in the top 10 results.
            \item \textbf{MAP} (Mean Average Precision): A summary statistic for precision across all recall levels, sensitive to the rank of all relevant documents.
            \item \textbf{MRR@10} (Mean Reciprocal Rank at cutoff 10): The average of the reciprocal of the rank of the first relevant document found. For this metric specifically, we evaluate on the MS MARCO v1.1 validation dataset from Huggingface \cite{DBLP:journals/corr/NguyenRSGTMD16} rather than TREC DL 2019, providing a complementary assessment on a larger, more diverse query set.
        \end{itemize}
    \item \textbf{Configurations Tested:}
        \begin{itemize}
            \item MiniLM + AdamW (Epochs 1, 2, 3)
            \item MiniLM + Lion (Epochs 1, 2, 3)
            \item GTE-multilingual-base + AdamW (Epochs 1, 2, 3)
            \item GTE-multilingual-base + Lion (Epochs 1, 2, 3)
            \item Modern-BERT-base + AdamW (epochs 1, 2, 3),
            \item Modern-BERT-base + Lion (repochs 1,2, 3)
        \end{itemize}
\end{itemize}

\section{Results and Discussion}
\label{sec:results}

Table \ref{tab:main_results} presents the primary evaluation results (NDCG@10, MAP, MRR@10, Recall@10, R-Prec, P@10) for the different model and optimizer configurations on the TREC 2019 Deep Learning Track passage ranking task after 1, 2, and 3 epochs of fine-tuning on MS MARCO.



\begin{table*}[htbp]
\centering
\caption{Main Evaluation Results on TREC-DL 2019 and MS-MARCO(DEV) Passage Ranking}
\label{tab:main_results}
\begin{tabular}{l l c c c c c c c}
\toprule
\textbf{Base Model} & \textbf{Optimizer} & \textbf{Epoch} & \textbf{NDCG@10} & \textbf{MAP} & \textbf{MRR@10} & \textbf{Recall@10} & \textbf{R-Prec} & \textbf{P@10} \\
\midrule
\multirow{3}{*}{MiniLM-L12-H384} & \multirow{3}{*}{AdamW}
    & 1 & 0.7008 & 0.4814 & 0.5828 & 0.1712 & 0.4899 & 0.8047 \\
    & & 2 & 0.7094 & 0.4891 & 0.5818 & 0.1715 & 0.5017 & 0.8093 \\
    & & 3 & 0.7127 & 0.4908 & 0.5826 & 0.1706 & 0.4962 & 0.8023 \\
\midrule
\multirow{3}{*}{MiniLM-L12-H384} & \multirow{3}{*}{Lion}
    & 1 & 0.7031 & 0.4858 & 0.5890 & 0.1698 & 0.4904 & 0.8070 \\
    & & 2 & 0.6916 & 0.4755 & 0.5942 & 0.1724 & 0.5041 & 0.8116 \\
    & & 3 & 0.6808 & 0.4706 & \cellcolor{yellow}\textbf{0.5988} & 0.1701 & 0.4923 & 0.8023 \\
\midrule
\multirow{3}{*}{GTE-multilingual-base} & \multirow{3}{*}{AdamW}
    & 1 & 0.7224 & 0.5005 & 0.5940 & 0.1733 & 0.4957 & 0.8140 \\
    & & 2 & 0.7203 & 0.4999 & 0.5942 & \cellcolor{yellow}\textbf{0.1733} & 0.5067 & 0.8163 \\
    & & 3 & 0.6902 & 0.4899 & 0.5972 & 0.1730 & 0.5069 & 0.8140 \\
\midrule
\multirow{3}{*}{GTE-multilingual-base} & \multirow{3}{*}{Lion}
    & 1 & 0.6785 & 0.4754 & 0.5854 & 0.1684 & 0.4849 & 0.7953 \\
    & & 2 & 0.6909 & 0.4921 & 0.5957 & 0.1721 & 0.5053 & 0.8140 \\
    & & 3 & 0.6904 & 0.4912 & 0.5931 & 0.1719 & 0.5041 & 0.8093 \\
    \midrule
\multirow{3}{*}{Modern-BERT-base} & \multirow{3}{*}{AdamW}
    & 1 & 0.7105 & 0.5066 & 0.5865 & 0.1678 & 0.5161 & 0.8163 \\
    & & 2 & 0.6839 & 0.4893 & 0.5885 & 0.1634 & 0.4946 & 0.7814 \\
    & & 3 & 0.6959 & 0.4971 & 0.5916 & 0.1623 & 0.5116 & 0.786 \\
\midrule
\multirow{3}{*}{Modern-BERT-base} & \multirow{3}{*}{Lion}
    & 1 & 0.7142 & \cellcolor{yellow}\textbf{0.5121} & 0.5834 & 0.1689 & 0.5148 & 0.8163 \\
    & & 2 & \cellcolor{yellow}\textbf{0.7225} & 0.5115 & \cellcolor{yellow}\textbf{0.5907} & 0.1732 & \cellcolor{yellow}\textbf{0.5183} & 0.8209 \\
    & & 3 & 0.7051 & 0.5020 & \cellcolor{yellow}\textbf{0.5988} & 0.1722 & 0.5102 & \cellcolor{yellow}\textbf{0.8256} \\
\bottomrule
\end{tabular}
\vspace{1em}\\
\footnotesize{* MRR@10 is calculated on MS-MARCO V1.1 Validation.All other metrices are calculated on TREC-DL-19 eval.}
\end{table*}


\subsection{Optimizer Comparison (MiniLM)}
Comparing AdamW and Lion directly for the MiniLM base model reveals interesting trends.
\begin{itemize}
    \item **Peak Performance:** AdamW consistently achieves slightly higher peak performance than Lion across NDCG@10 and MAP metrics (NDCG@10: 0.7127 vs 0.7031, MAP: 0.4908 vs 0.4858), while Lion achieves the highest MRR@10 of 0.5988 on the MS MARCO validation set at Epoch 3.
    \item **Epoch Progression:** With AdamW, MiniLM's performance generally improves or stabilizes from Epoch 1 to Epoch 3 on TREC DL 2019 metrics. In contrast, with Lion, MiniLM's TREC performance peaks at Epoch 1 and then degrades slightly in Epochs 2 and 3, though its MS MARCO MRR@10 continues to improve. This might suggest that the chosen learning rate (2e-6) and schedule are better suited for AdamW over 3 epochs, or that Lion converges faster on TREC metrics but continues to improve on MS MARCO.
\end{itemize}
These results suggest that for MiniLM on this task, AdamW yielded slightly better effectiveness, particularly when training for multiple epochs. Lion's faster convergence might be advantageous if training time or compute resources are highly constrained, but achieving the best possible score favoured AdamW here.

\subsection{Model Comparison (AdamW)}
Comparing MiniLM and GTE-multilingual-base, both trained with AdamW:
\begin{itemize}
    \item GTE generally outperforms MiniLM, especially in the first epoch (e.g., NDCG@10 0.7224 for GTE vs 0.7008 for MiniLM).
    \item GTE's peak performance (Epoch 2: NDCG@10=0.7203, MAP=0.4999) on TREC DL 2019 is higher than MiniLM's best (Epoch 3: NDCG@10=0.7127, MAP=0.4908).
    \item For MRR@10 on MS MARCO validation, GTE achieves its best performance at Epoch 3 (0.5972), similar to MiniLM with Lion.
    \item GTE shows a clearer peak performance at Epoch 2 for TREC metrics, with a noticeable drop in Epoch 3, potentially indicating earlier overfitting on the TREC benchmark compared to MiniLM with AdamW.
\end{itemize}
Overall, the GTE-multilingual-base model, when trained with AdamW, provided the best reranking effectiveness in our experiments, peaking after the second epoch.

\subsection{Performance Trends Over Epochs}
Figures \ref{fig:ndcg_vs_epoch}, \ref{fig:map_vs_epoch}, and \ref{fig:mrr_vs_epoch} visualize the performance trends across epochs for the different configurations.

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{placeholder_ndcg.png}
\caption{NDCG@10 vs. Training Epoch}
\label{fig:ndcg_vs_epoch}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{placeholder_map.png}
\caption{MAP vs. Training Epoch}
\label{fig:map_vs_epoch}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{placeholder_mrr.png}
\caption{MRR (recip\_rank) vs. Training Epoch}
\label{fig:mrr_vs_epoch}
\end{figure}

The plots reinforce the observations from Table \ref{tab:main_results}. MiniLM+AdamW shows gradual improvement or stability. MiniLM+Lion peaks early and declines. GTE+AdamW peaks at Epoch 2 and then declines more significantly than MiniLM+AdamW. This highlights the importance of monitoring performance across epochs and potentially using early stopping based on a validation set (like the MS MARCO dev set or a held-out portion of TREC queries if available) to select the best checkpoint, rather than just training for a fixed number of epochs.

\subsection{Comparison with State-of-the-Art}
To contextualize our results, Table \ref{tab:sota_comparison} compares our best-performing model (GTE-multilingual-base + AdamW, Epoch 2) with other reported results on the TREC 2019 Deep Learning passage ranking task.

\begin{table}[htbp]
\centering
\caption{Comparison with State-of-the-Art on TREC DL 2019}
\label{tab:sota_comparison}
\begin{tabular}{l c c}
\toprule
\textbf{Model/System} & \textbf{NDCG@10} & \textbf{Reference} \\
\midrule
% Single Vector Dense Retrieval Models & BM25
BM25 (Pyserini baseline) & 0.531 & \cite{lin2021pyserini} \\
ANCE & 0.646 & \cite{xiong2021approximate} \\
TCT-ColBERT & 0.669 & \cite{lin2021batch} \\
Margin MSE & 0.669 & \cite{hofstatter2021efficiently} \\
TAS-B & 0.700 & \cite{hofstatter2021efficiently} \\
CL-DRD & 0.701 & \cite{yu2022improving} \\
BE-Base & 0.713 & \cite{zhan2024scaling} \\
ms-marco-TinyBERT-L2-v2 & 69.84 & \cite{reimers-2019-sentence-bert} \\
ms-marco-MiniLM-L2-v2 & 71.01 & \cite{reimers-2019-sentence-bert} \\
% koursaros2019NBoost, nogueira2020passagererankingbert
pt-tinybert-msmarco & 63.63 & \cite{koursaros2019NBoost} \\
pt-bert-base-uncased-msmarco & 70.94 & \cite{koursaros2019NBoost} \\
bert-multilingual-passage-reranking-msmarco & 68.40 & \cite{nogueira2020passagererankingbert} \\

\midrule
% Cross-encoders and other rerankers
BERT-base Cross-Encoder & 0.634 & \cite{Lin2021PretrainedTF} \\
BERT-large Cross-Encoder & 0.654 & \cite{Lin2021PretrainedTF} \\
MonoT5-3B & 0.741 & \cite{nogueira2021monot5} \\
ColBERT (Late-Interaction) & 0.743 & \cite{khattab2020colbert} \\
\midrule
\textbf{Our Best (modernBERT+Lion E2)} & \textbf{0.7225} & This work \\
\bottomrule
\end{tabular}
\vspace{1em}\\
\footnotesize{BM25 baseline from Pyserini, BERT metrics from Lin et al. (2021), MonoT5-3B from Nogueira et al. (2021), ColBERT from Khattab et al. (2020). Dense retrieval model metrics as reported in Zhan et al. (2024) \cite{zhan2024scaling}.}
\end{table}

Our best result of NDCG@10 = 0.7203 surpasses the BM25 baseline by 35 points and outperforms the BERT-base cross-encoder. It is competitive with the BERT-large cross-encoder, though slightly below the state-of-the-art MonoT5-3B reranker and ColBERT (Late-Interaction). This demonstrates that our GTE+AdamW model achieves near top-tier performance with a smaller model footprint.

\section{Conclusion}
\label{sec:conclusion}
In this paper, we evaluated the effectiveness of the Lion optimizer compared to the standard AdamW optimizer for fine-tuning cross-encoder models (`microsoft/MiniLM-L12-H384-uncased` and `Alibaba-NLP/gte-multilingual-base`) for passage reranking. Using the MS MARCO dataset for training and the TREC 2019 Deep Learning Track for evaluation, we found that:
\begin{itemize}
    \item For the MiniLM model, AdamW slightly outperformed Lion in terms of peak effectiveness (NDCG@10, MAP, MRR), particularly when training for 3 epochs. Lion showed faster initial convergence but performance degraded slightly after the first epoch.
    \item The GTE-multilingual-base model generally outperformed MiniLM when using the AdamW optimizer.
    \item The best overall performance in our experiments (NDCG@10=0.7203, MAP=0.4999) was achieved by GTE-multilingual-base fine-tuned with AdamW for 2 epochs.
    \item Performance varied significantly across epochs, highlighting the need for careful checkpoint selection or early stopping.
\end{itemize}

Our results suggest that while Lion is a viable optimizer for training cross-encoders, AdamW might still hold a slight edge in achieving maximum effectiveness for this specific task and setup, at least with the hyperparameters explored. The GTE model proved to be a strong base for reranking. The use of the Modal platform greatly facilitated managing the computational experiments.

Future work could involve more extensive hyperparameter tuning for the Lion optimizer (especially learning rate and $\beta$ values), exploring different learning rate schedules, evaluating on other IR benchmarks (e.g., TREC DL 2020/2021), and analyzing the training stability and memory efficiency differences between the optimizers in more detail. Comparing with other recent optimizers could also yield further insights.

\section*{Acknowledgment}
The authors acknowledge Modal Labs (\url{https://modal.com/}) for providing the cloud computing platform and GPU resources used for conducting the experiments presented in this paper.

% --- Bibliography ---
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references} % references.bib should contain your BibTeX entries

\end{document}