\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs} % For better tables
\usepackage{multirow} % For multirow cells in tables
\usepackage{url}      % For URLs
\usepackage{hyperref} % For clickable links (optional)

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Define colors for comments or notes (optional)
\usepackage{xcolor}
\newcommand{\todo}[1]{\textcolor{purple}{[TODO: #1]}}
\newcommand{\note}[1]{\textcolor{teal}{[Note: #1]}}


\begin{document}

\title{Scaling Up Cross-Encoder Reranking with 8K Token Inputs: A Comparative Study of Lion and AdamW on TREC DL 2019}

% --- Author Information ---
% Use \and to separate authors if more than one
\author{\IEEEauthorblockN{Shahil Kumar} % Replace with author names
\IEEEauthorblockA{\textit{IT} \\ % Replace with affiliation
\textit{IIIT Allahabad}\\
Prayagraj, India \\
abc@email.com} % Replace with email
\and % Uncomment and repeat for additional authors
\IEEEauthorblockN{Author 2 Name}
\IEEEauthorblockA{\textit{Department} \\
\textit{Institution}\\
City, Country \\
email address}
}
% --- End Author Information ---

\maketitle

\begin{abstract}
Modern information retrieval systems often employ a two-stage pipeline consisting of an efficient initial retrieval stage followed by a more computationally intensive reranking stage. Cross-encoder models have demonstrated state-of-the-art effectiveness for the reranking task due to their ability to perform deep, contextualized analysis of query-document pairs. The choice of optimizer during the fine-tuning phase can significantly impact the final performance and training efficiency of these models. This paper investigates the impact of using the recently proposed Lion optimizer compared to the widely used AdamW optimizer for fine-tuning cross-encoder rerankers. We fine-tune two popular transformer models, `microsoft/MiniLM-L12-H384-uncased` and `Alibaba-NLP/gte-multilingual-base`, on the MS MARCO passage ranking dataset using both optimizers. The effectiveness of the resulting models is evaluated on the TREC 2019 Deep Learning Track passage reranking task. Our experiments, facilitated by the Modal cloud computing platform for GPU offloading, show comparative results across different epochs. We analyze the performance trends based on standard IR metrics like NDCG@10, MAP, and MRR, providing insights into the effectiveness of Lion versus AdamW in this specific reranking context.
\end{abstract}

\begin{IEEEkeywords}
Information Retrieval, Cross-Encoder, Reranking, Optimizer, Lion Optimizer, AdamW, TREC Deep Learning, MS MARCO, Sentence Transformers, Modal.
\end{IEEEkeywords}

\section{Introduction}
Information Retrieval (IR) systems aim to satisfy a user's information need, often expressed as a textual query, by returning a ranked list of relevant documents from a large collection. A common and effective architecture for modern search systems is the two-stage retrieve-and-rerank pipeline \cite{Lin2021PretrainedTF}. The first stage employs computationally efficient methods, such as BM25 \cite{robertson2009probabilistic} or dense vector retrieval \cite{karpukhin2020dense}, to quickly retrieve a candidate set of documents (typically hundreds or thousands) from the entire collection. The second stage then applies a more sophisticated and computationally expensive reranking model to reorder this smaller candidate set, aiming for higher precision at the top ranks.

Cross-encoder models, typically based on transformer architectures like BERT \cite{devlin2019bert}, have emerged as state-of-the-art rerankers \cite{nogueira2019passage, Nogueira2020Document}. Unlike bi-encoder models that encode queries and documents independently, cross-encoders process the query and a candidate document simultaneously (e.g., `[CLS] query [SEP] document [SEP]`). This allows for deep, token-level interaction modeling, leading to superior relevance estimation accuracy, albeit at a higher computational cost suitable only for the second stage.

Fine-tuning these large transformer models effectively is crucial. The choice of optimizer plays a key role in navigating the complex loss landscape and achieving optimal performance. Adam \cite{kingma2014adam} and its variant AdamW \cite{loshchilov2019decoupled} are widely used and generally effective optimizers for training deep neural networks. Recently, the Lion (Evo\textbf{L}ved S\textbf{i}gn M\textbf{o}me\textbf{n}tum) optimizer \cite{chen2023symbolic} was proposed, derived through symbolic mathematics and program search. It claims improved performance and memory efficiency compared to AdamW on various tasks, particularly image classification and vision-language models.

In this work, we investigate the applicability and effectiveness of the Lion optimizer for fine-tuning cross-encoder models specifically for the task of passage reranking in information retrieval. We compare its performance against the standard AdamW optimizer. Our contributions are:
\begin{itemize}
    \item We fine-tune two distinct transformer-based cross-encoder models (`microsoft/MiniLM-L12-H384-uncased` and `Alibaba-NLP/gte-multilingual-base`) on the large-scale MS MARCO passage dataset \cite{bajaj2016ms} using both Lion and AdamW optimizers.
    \item We evaluate the performance of the fine-tuned models across three training epochs on the TREC 2019 Deep Learning (DL) Track passage reranking benchmark \cite{craswell2020overview}.
    \item We provide a comparative analysis of the optimizers' impact on reranking effectiveness using standard IR metrics (NDCG@10, MAP, MRR).
    \item We utilize the Modal cloud platform \cite{modal_labs} for efficient GPU resource management and reproducible experimentation.
\end{itemize}

The rest of the paper is organized as follows: Section \ref{sec:related_work} discusses related work. Section \ref{sec:methodology} details the models, optimizers, and training approach. Section \ref{sec:experimental_setup} describes the experimental setup, datasets, and evaluation protocol. Section \ref{sec:results} presents and discusses the results. Finally, Section \ref{sec:conclusion} concludes the paper and suggests future work.

\section{Related Work}
\label{sec:related_work}

\subsection{Cross-Encoder Reranking}
The use of BERT-based models for document reranking was popularized by Nogueira et al. \cite{nogueira2019passage, Nogueira2020Document}. They demonstrated that fine-tuning BERT as a cross-encoder on relevance labels (like those from MS MARCO) significantly outperforms traditional IR models and even bi-encoder approaches for reranking tasks. Subsequent work explored various transformer architectures \cite{Lin2021PretrainedTF}, training strategies \cite{gao2021complementing}, and efficiency improvements \cite{hofstatter2020improving}. Models like MiniLM \cite{wang2020minilm} offer a balance between effectiveness and efficiency, while multilingual models like GTE \cite{li2023towards} (General Text Embeddings) provide strong performance across various text tasks, including retrieval, and can be adapted for cross-encoding. The `sentence-transformers` library \cite{reimers2019sentence} provides a convenient framework for training and using both bi-encoders and cross-encoders.

\subsection{Optimizers for Deep Learning}
Stochastic Gradient Descent (SGD) with momentum remains a fundamental optimizer, but adaptive learning rate methods like AdaGrad \cite{duchi2011adaptive}, RMSprop \cite{tieleman2012lecture}, and Adam \cite{kingma2014adam} often lead to faster convergence in practice for deep learning models. Adam combines momentum with adaptive scaling of learning rates based on estimates of first and second moments of the gradients. AdamW \cite{loshchilov2019decoupled} improves upon Adam by decoupling the weight decay regularization from the adaptive learning rate mechanism, often leading to better generalization. The Lion optimizer \cite{chen2023symbolic} takes a different approach, using only momentum tracking and a sign operation on the update, resulting in a simpler update rule (`update = sign(momentum) * lr`) and potentially requiring less memory due to not storing second moment estimates. Its effectiveness relative to AdamW has been shown primarily in vision and vision-language tasks, motivating its evaluation in the NLP/IR domain.

\subsection{Evaluation Benchmarks}
The MS MARCO (Microsoft MAchine Reading COmprehension) dataset \cite{bajaj2016ms} has become a standard benchmark for training and evaluating deep learning models for passage retrieval and reranking. It contains real user queries from Bing and human-judged relevant passages. The TREC Deep Learning (DL) Tracks \cite{craswell2020overview, craswell2021overview} provide challenging test collections built upon MS MARCO, using queries with sparse relevance judgments derived from pooling top results from various participating systems. We use the TREC DL 2019 passage ranking dataset, a standard benchmark for evaluating reranking effectiveness. Evaluation is typically performed using tools like `trec\_eval` \cite{trec_eval_github}, which calculates various standard IR metrics.

\section{Methodology}
\label{sec:methodology}

\subsection{Cross-Encoder Architecture}
A cross-encoder model takes a query $q$ and a document $d$ as input, typically concatenating them with special tokens: `[CLS] q [SEP] d [SEP]`. This combined sequence is fed into a transformer model (e.g., BERT, MiniLM, GTE). The output representation corresponding to the `[CLS]` token is then passed through a linear layer followed by a sigmoid activation (for binary classification) or used directly (for regression) to predict a relevance score $s(q, d)$. During inference for reranking, this score is computed for all candidate documents retrieved in the first stage, and the documents are re-sorted based on these scores.

\subsection{Base Models}
We experiment with two transformer base models:
\begin{itemize}
    \item \textbf{`microsoft/MiniLM-L12-H384-uncased` \cite{wang2020minilm}:} A distilled version of BERT, designed to be smaller and faster while retaining significant performance. It uses knowledge distillation from a larger teacher model during pre-training. It has 12 layers and a hidden size of 384.
    \item \textbf{`Alibaba-NLP/gte-multilingual-base` \cite{li2023towards}:} Part of the General Text Embeddings (GTE) family, trained on a large, diverse corpus covering multiple domains and languages. While often used as a bi-encoder for text embeddings, its underlying transformer architecture can be effectively fine-tuned as a cross-encoder. This is the 'base' sized version.
\end{itemize}

\subsection{Training}
We fine-tune the cross-encoders using the MS MARCO passage ranking triplets dataset, as processed by `sentence-transformers` \cite{reimers2019sentence}. The original dataset contains tuples of (query, positive passage, negative passage). We convert this into pairs `(query, positive\_passage)` with label 1 and `(query, negative\_passage)` with label 0.
The training objective is to minimize the Binary Cross-Entropy (BCE) loss between the predicted relevance score (output of the sigmoid function) and the true label (0 or 1). The BCE loss is defined as:
$$ \mathcal{L}_{BCE} = - [y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})] $$
where $y$ is the true label and $\hat{y}$ is the model's predicted probability.

\subsection{Optimizers}
We compare two optimizers:
\begin{itemize}
    \item \textbf{AdamW \cite{loshchilov2019decoupled}:} A widely adopted optimizer that modifies the original Adam algorithm to decouple weight decay from the gradient update step, often leading to improved regularization and generalization.
    \item \textbf{Lion \cite{chen2023symbolic}:} Discovered through automated symbolic search, Lion maintains only momentum and uses the sign of the momentum for the update direction. Its update rule is simpler than AdamW and does not require second moment estimates, potentially making it more memory-efficient. The update involves:
    $ m_t \leftarrow \beta_1 m_{t-1} + (1 - \beta_1) g_t $
    $ \theta_t \leftarrow \theta_{t-1} - \eta \cdot \text{sign}(\beta_2 m_t + (1 - \beta_2) g_t) $
    where $g_t$ is the gradient, $\eta$ is the learning rate, and $\beta_1, \beta_2$ are momentum coefficients. Note: The paper uses $\beta_1=0.9, \beta_2=0.99$ as defaults, but implementations might vary slightly. Our code uses the `lion-pytorch` implementation which appears consistent with the paper's description.
\end{itemize}

\section{Experimental Setup}
\label{sec:experimental_setup}

\subsection{Datasets}
\begin{itemize}
    \item \textbf{Training:} We use the MS MARCO passage ranking triplets dataset \cite{bajaj2016ms}, processed into query-passage pairs with binary labels. We utilize the data loading strategy from `sentence-transformers`, sampling approximately 2 million training pairs derived from 1 million triplets, as specified in the `trainer.py` script. A small subset (10,000 pairs) is held out for validation during training, primarily for checkpoint selection based on loss, although final evaluation uses the TREC DL benchmark.
    \item \textbf{Evaluation:} We evaluate on the TREC 2019 Deep Learning Track passage ranking task \cite{craswell2020overview}. This dataset contains 43 queries with graded relevance judgments (qrels) for passages from the MS MARCO corpus.
\end{itemize}

\subsection{Implementation Details}
\begin{itemize}
    \item \textbf{Framework:} We use the `sentence-transformers` library \cite{reimers2019sentence} (specifically `CrossEncoder` and `CrossEncoderTrainer`) built on top of PyTorch and Hugging Face Transformers \cite{wolf2020transformers}.
    \item \textbf{Hyperparameters:} Based on the provided `trainer.py` and `modal\_offload.py`, key training parameters include:
        \begin{itemize}
            \item Batch Size: 512 (per device)
            \item Learning Rate: 2e-6 (for Lion; AdamW typically uses a similar range, e.g., 2e-5 or 2e-6)
            \item Optimizer Betas (Lion): (0.9, 0.99)
            \item Warmup Ratio: 0.1
            \item Epochs: 3
            \item Precision: BF16 is enabled (`bf16=True`).
            \item Max Sequence Length: 1024 tokens (specified in `evaluate` function call in `modal\_offload.py`, assumed used for training too for consistency, although `trainer.py` doesn't explicitly set it, relying on model's default or internal setting).
            \item Seed: 12 (for reproducibility).
        \end{itemize}
    \item \textbf{Infrastructure:} Experiments were conducted using the Modal platform \cite{modal_labs}, leveraging NVIDIA A100-80GB GPUs for training and evaluation. Modal facilitated environment management (CUDA 12.4, Python 3.11, required libraries) and offloading computation to the cloud. Code scripts (`modal\_offload.py`, `trec\_modal.py`, `trainer.py`, `trec\_dl\_19\_eval\_2.py`) define the Modal functions and dependencies.
    \item \textbf{First-Stage Retrieval:} For the TREC DL evaluation, we first retrieve the top 1000 candidate passages per query using a standard BM25 baseline implemented with Pyserini \cite{lin2021pyserini} on the `msmarco-v1-passage` pre-built index. This constitutes the candidate set for reranking.
\end{itemize}

\subsection{Evaluation Protocol}
\begin{itemize}
    \item \textbf{Reranking:} Each fine-tuned cross-encoder model is used to score the top-1000 BM25 candidates for each TREC DL 2019 query. The passages are then reranked based on these cross-encoder scores.
    \item \textbf{Metrics:} We use the official `trec\_eval` tool \cite{trec_eval_github} to compute effectiveness metrics against the official TREC DL 2019 qrels. We report the following key metrics (though the provided JSON files contain many more):
        \begin{itemize}
            \item \textbf{NDCG@10} (Normalized Discounted Cumulative Gain at cutoff 10): Measures graded relevance and ranking quality in the top 10 results.
            \item \textbf{MAP} (Mean Average Precision): A summary statistic for precision across all recall levels, sensitive to the rank of all relevant documents.
            \item \textbf{MRR} (Mean Reciprocal Rank): The average of the reciprocal of the rank of the first relevant document found. Sensitive to finding the first correct answer quickly. Also reported as `recip\_rank` in `trec\_eval`.
        \end{itemize}
    \item \textbf{Configurations Tested:}
        \begin{itemize}
            \item MiniLM + AdamW (Epochs 1, 2, 3)
            \item MiniLM + Lion (Epochs 1, 2, 3)
            \item GTE-multilingual-base + AdamW (Epochs 1, 2, 3)
            % \item \todo{Add GTE + Lion if results are available}
        \end{itemize}
\end{itemize}

\section{Results and Discussion}
\label{sec:results}

Table \ref{tab:main_results} presents the primary evaluation results (NDCG@10, MAP, MRR) for the different model and optimizer configurations on the TREC 2019 Deep Learning Track passage ranking task after 1, 2, and 3 epochs of fine-tuning on MS MARCO.

\begin{table*}[htbp]
\centering
\caption{Main Evaluation Results on TREC DL 2019 Passage Ranking}
\label{tab:main_results}
\begin{tabular}{l l c c c c c c c}
\toprule
\textbf{Base Model} & \textbf{Optimizer} & \textbf{Epoch} & \textbf{NDCG@10} & \textbf{MAP} & \textbf{MRR} & \textbf{Recall@10} & \textbf{R-Prec} & \textbf{P@10} \\
\midrule
\multirow{3}{*}{MiniLM-L12-H384} & \multirow{3}{*}{AdamW}
    & 1 & 0.7008 & 0.4814 & 0.9465 & 0.1712 & 0.4899 & 0.8047 \\
    & & 2 & 0.7094 & 0.4891 & \textbf{0.9814} & 0.1715 & 0.5017 & 0.8093 \\
    & & 3 & 0.7127 & 0.4908 & 0.9806 & 0.1706 & 0.4962 & 0.8023 \\
\midrule
\multirow{3}{*}{MiniLM-L12-H384} & \multirow{3}{*}{Lion}
    & 1 & 0.7031 & 0.4858 & 0.9457 & 0.1698 & 0.4904 & 0.8070 \\
    & & 2 & 0.6916 & 0.4755 & 0.9496 & 0.1724 & 0.5041 & 0.8116 \\
    & & 3 & 0.6808 & 0.4706 & 0.9419 & 0.1701 & 0.4923 & 0.8023 \\
\midrule
\multirow{3}{*}{GTE-multilingual-base} & \multirow{3}{*}{AdamW}
    & 1 & \textbf{0.7224} & \textbf{0.5005} & 0.9814 & 0.1733 & 0.4957 & 0.8140 \\
    & & 2 & {0.7203} & 0.4999 & 0.9523 & \textbf{0.1733} & \textbf{0.5067} & \textbf{0.8163} \\
    & & 3 & 0.6902 & 0.4899 & 0.9279 & 0.1730 & 0.5069 & 0.8140 \\
\midrule
\multirow{3}{*}{GTE-multilingual-base} & \multirow{3}{*}{Lion}
    & 1 & 0.6785 & 0.4754 & 0.9244 & 0.1684 & 0.4849 & 0.7953 \\
    & & 2 & {0.6909} & {0.4921} & {0.9419} & {0.1721} & {0.5053} & {0.8140} \\
    & & 3 & {0.6904} & {0.4912} & {0.9477} & {0.1719} & {0.5041} & {0.8093} \\
\bottomrule
\end{tabular}
\vspace{1em}\\
\footnotesize{* MRR corresponds to the `recip\_rank` metric in `trec\_eval` output. Recall@10, R-Prec, and P@10 added for broader evaluation coverage.}
\end{table*}

\subsection{Optimizer Comparison (MiniLM)}
Comparing AdamW and Lion directly for the MiniLM base model reveals interesting trends.
\begin{itemize}
    \item **Peak Performance:** AdamW consistently achieves slightly higher peak performance than Lion across all three metrics (NDCG@10: 0.7127 vs 0.7031, MAP: 0.4908 vs 0.4858, MRR: 0.9814 vs 0.9496, comparing best epoch for each).
    \item **Epoch Progression:** With AdamW, MiniLM's performance generally improves or stabilizes from Epoch 1 to Epoch 3. In contrast, with Lion, MiniLM's performance peaks at Epoch 1 and then degrades slightly in Epochs 2 and 3. This might suggest that the chosen learning rate (2e-6) and schedule are better suited for AdamW over 3 epochs, or that Lion converges faster and might benefit from earlier stopping or a different learning rate schedule for this specific task and model.
\end{itemize}
These results suggest that for MiniLM on this task, AdamW yielded slightly better effectiveness, particularly when training for multiple epochs. Lion's faster convergence might be advantageous if training time or compute resources are highly constrained, but achieving the best possible score favoured AdamW here.

\subsection{Model Comparison (AdamW)}
Comparing MiniLM and GTE-multilingual-base, both trained with AdamW:
\begin{itemize}
    \item GTE generally outperforms MiniLM, especially in the first epoch (e.g., NDCG@10 0.7224 for GTE vs 0.7008 for MiniLM).
    \item GTE's peak performance (Epoch 2: NDCG@10=0.7203, MAP=0.4999, MRR=0.9523) is higher than MiniLM's best (Epoch 3: NDCG@10=0.7127, MAP=0.4908, MRR=0.9806), particularly on MAP, although MiniLM achieves a slightly higher peak MRR.
    \item GTE shows a clearer peak performance at Epoch 2, with a noticeable drop in Epoch 3, potentially indicating earlier overfitting compared to MiniLM with AdamW.
\end{itemize}
Overall, the GTE-multilingual-base model, when trained with AdamW, provided the best reranking effectiveness in our experiments, peaking after the second epoch.

\subsection{Performance Trends Over Epochs}
Figures \ref{fig:ndcg_vs_epoch}, \ref{fig:map_vs_epoch}, and \ref{fig:mrr_vs_epoch} visualize the performance trends across epochs for the different configurations.

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{placeholder_ndcg.png} % Replace with actual generated plot
\caption{NDCG@10 vs. Training Epoch}
\label{fig:ndcg_vs_epoch}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{placeholder_map.png} % Replace with actual generated plot
\caption{MAP vs. Training Epoch}
\label{fig:map_vs_epoch}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{placeholder_mrr.png} % Replace with actual generated plot
\caption{MRR (recip\_rank) vs. Training Epoch}
\label{fig:mrr_vs_epoch}
\end{figure}

The plots reinforce the observations from Table \ref{tab:main_results}. MiniLM+AdamW shows gradual improvement or stability. MiniLM+Lion peaks early and declines. GTE+AdamW peaks at Epoch 2 and then declines more significantly than MiniLM+AdamW. This highlights the importance of monitoring performance across epochs and potentially using early stopping based on a validation set (like the MS MARCO dev set or a held-out portion of TREC queries if available) to select the best checkpoint, rather than just training for a fixed number of epochs.

\subsection{Comparison with State-of-the-Art (Placeholder)}
To contextualize our results, Table \ref{tab:sota_comparison} compares our best-performing model (GTE-multilingual-base + AdamW, Epoch 2) with other reported results on the TREC 2019 Deep Learning passage ranking task.

\begin{table}[htbp]
\centering
\caption{Comparison with State-of-the-Art on TREC DL 2019}
\label{tab:sota_comparison}
\begin{tabular}{l c c}
\toprule
\textbf{Model/System} & \textbf{NDCG@10} & \textbf{Reference} \\
\midrule
BM25 (Pyserini baseline) & \textit{\todo{Find value}} & \cite{lin2021pyserini} \\ % Need to run BM25 only eval or find reported value
BERT-base Cross-Encoder & \textit{\todo{Find value}} & \cite{nogueira2019passage} / TREC Overview \\
BERT-large Cross-Encoder & \textit{\todo{Find value}} & \cite{nogueira2019passage} / TREC Overview \\
\textit{Other SOTA Rerankers} & \textit{\todo{Add values}} & \textit{\todo{Cite sources}} \\
\midrule
\textbf{Our Best (GTE+AdamW E2)} & \textbf{0.7203} & This work \\
\bottomrule
\end{tabular}
\vspace{1em} % Add some space after the table
\footnotesize{* Values for other systems need to be sourced from relevant publications or TREC overview papers.}
\end{table}

\todo{Discuss how your best model compares to the SOTA once the table is filled. Is it competitive? Are there significant differences?} Our best result of NDCG@10 = 0.7203 appears competitive with strong cross-encoder baselines, although direct comparison requires careful consideration of base model size, training data, and exact hyperparameters used in other studies.

\section{Conclusion}
\label{sec:conclusion}
In this paper, we evaluated the effectiveness of the Lion optimizer compared to the standard AdamW optimizer for fine-tuning cross-encoder models (`microsoft/MiniLM-L12-H384-uncased` and `Alibaba-NLP/gte-multilingual-base`) for passage reranking. Using the MS MARCO dataset for training and the TREC 2019 Deep Learning Track for evaluation, we found that:
\begin{itemize}
    \item For the MiniLM model, AdamW slightly outperformed Lion in terms of peak effectiveness (NDCG@10, MAP, MRR), particularly when training for 3 epochs. Lion showed faster initial convergence but performance degraded slightly after the first epoch.
    \item The GTE-multilingual-base model generally outperformed MiniLM when using the AdamW optimizer.
    \item The best overall performance in our experiments (NDCG@10=0.7203, MAP=0.4999) was achieved by GTE-multilingual-base fine-tuned with AdamW for 2 epochs.
    \item Performance varied significantly across epochs, highlighting the need for careful checkpoint selection or early stopping.
\end{itemize}

Our results suggest that while Lion is a viable optimizer for training cross-encoders, AdamW might still hold a slight edge in achieving maximum effectiveness for this specific task and setup, at least with the hyperparameters explored. The GTE model proved to be a strong base for reranking. The use of the Modal platform greatly facilitated managing the computational experiments.

Future work could involve more extensive hyperparameter tuning for the Lion optimizer (especially learning rate and $\beta$ values), exploring different learning rate schedules, evaluating on other IR benchmarks (e.g., TREC DL 2020/2021), and analyzing the training stability and memory efficiency differences between the optimizers in more detail. Comparing with other recent optimizers could also yield further insights.

\section*{Acknowledgment}
The authors acknowledge Modal Labs (\url{https://modal.com/}) for providing the cloud computing platform and GPU resources used for conducting the experiments presented in this paper.

% --- Bibliography ---
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,references} % references.bib should contain your BibTeX entries

\end{document}