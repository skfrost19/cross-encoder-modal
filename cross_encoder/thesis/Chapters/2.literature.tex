\chapter{Literature Review}
\label{Chapter2}

This chapter reviews key literature on cross-encoder models, optimization algorithms, and evaluation methodologies for information retrieval.

\section{Neural Information Retrieval Evolution}

Information retrieval has transformed from traditional term-based matching (TF-IDF, BM25 \cite{robertson2009probabilistic}) to neural approaches that capture semantic relationships. Dense retrieval methods like DPR \cite{karpukhin2020dense} and ANCE \cite{xiong2020approximate} encode queries and documents independently, while cross-encoders enable richer query-document interactions.

\section{Cross-Encoder Architectures}

Cross-encoders leverage transformer architectures for document reranking. Nogueira and Cho \cite{nogueira2020passagererankingbert} demonstrated BERT's effectiveness for reranking by concatenating queries and documents with special tokens. The [CLS] representation predicts relevance scores through fine-tuning on labeled data.

Modern architectures include:
\begin{itemize}
\item \textbf{MiniLM \cite{wang2020minilm}}: Distilled BERT variant for efficiency
\item \textbf{GTE \cite{li2023towards}}: Multilingual model with extended context (8192 tokens)  
\item \textbf{ModernBERT \cite{modernbert}}: Advanced architecture with RoPE, Flash Attention, GeGLU
\end{itemize}

\section{Optimization Algorithms}

\subsection{Traditional Optimizers}
Adam \cite{kingma2017adam} combines momentum with adaptive learning rates using first and second moment estimates. AdamW \cite{loshchilov2019decoupled} improved generalization by decoupling weight decay from gradient updates, becoming the standard for transformer training.

\subsection{Lion Optimizer}
Lion \cite{chen2023symbolic} represents a paradigm shift using sign-based momentum: \texttt{update = sign(momentum) * learning\_rate}. This simplified approach reduces memory requirements while maintaining competitive performance across vision tasks.

\section{Evaluation Benchmarks}

Standard IR evaluation relies on:
\begin{itemize}
\item \textbf{MS MARCO \cite{DBLP:journals/corr/NguyenRSGTMD16}}: Large-scale passage ranking dataset
\item \textbf{TREC DL 2019 \cite{craswell2020overview}}: Deep learning track for passage ranking
\item \textbf{Metrics}: nDCG@10, MRR@10, MAP measure ranking quality
\end{itemize}

\section{Summary}

This literature review establishes the foundation for understanding cross-encoder architectures, optimization algorithms, and evaluation methodologies in information retrieval. The reviewed work demonstrates the effectiveness of transformer-based cross-encoders and the dominance of AdamW as the standard optimizer. However, the systematic exploration of alternative optimization strategies, particularly the recently proposed Lion optimizer, remains an open area for investigation in the context of information retrieval tasks.

%----------------------------------------------------------------------------------------




