% Chapter Template

\chapter{Summary \& Future Scope of Work}
\label{Chapter6}

This chapter summarizes the key contributions of our research and outlines promising directions for future investigation in optimizer effectiveness for cross-encoder reranking.

\section{Research Summary}

This thesis investigated the comparative effectiveness of Lion and AdamW optimizers for cross-encoder reranking across three transformer architectures: MiniLM, GTE, and ModernBERT.

\subsection{Primary Contributions}

\begin{enumerate}
    \item \textbf{First Systematic Evaluation}: Conducted the first comprehensive comparison of Lion optimizer for cross-encoder information retrieval tasks across multiple architectures.
    
    \item \textbf{Architecture-Optimizer Interaction Analysis}: Demonstrated that optimizer effectiveness varies significantly based on model architecture and training configuration.
    
    \item \textbf{State-of-the-art Results}: Achieved best-in-class performance with ModernBERT + Lion (NDCG@10: 0.7225) and tied best MRR@10 (0.5988) on standard IR benchmarks.
    
    \item \textbf{Training Dynamics Insights}: Provided detailed analysis of convergence patterns and gradient behavior differences between optimizers.
\end{enumerate}

\subsection{Key Findings Summary}

Our experimental results establish that:
\begin{itemize}
    \item \textbf{No Universal Optimizer}: Neither Lion nor AdamW consistently outperforms across all architectures
    \item \textbf{Configuration Sensitivity}: Optimizer effectiveness depends heavily on learning rate and scheduling choices
    \item \textbf{Modern Architecture Benefits}: Advanced transformer features synergize well with Lion's optimization approach
    \item \textbf{Metric-Dependent Performance}: Different optimizers excel on different evaluation metrics
\end{itemize}

\section{Future Scope of Work}

This research opens several promising avenues for future investigation:

\subsection{Immediate Extensions}

\begin{enumerate}
    \item \textbf{Comprehensive Hyperparameter Studies}:
    \begin{itemize}
        \item Systematic grid search for Lion's optimal parameters across model architectures
        \item Investigation of alternative learning rate schedules beyond Cosine Annealing
        \item Analysis of momentum parameter sensitivity
    \end{itemize}

    \item \textbf{Extended Architecture Coverage}:
    \begin{itemize}
        \item Evaluation on larger model sizes (large and extra-large variants)
        \item Testing on emerging transformer architectures (e.g., Mamba, RetNet)
        \item Investigation of decoder-only models for reranking tasks
    \end{itemize}

    \item \textbf{Enhanced Evaluation Scope}:
    \begin{itemize}
        \item Extended evaluation on additional IR benchmarks (BEIR, LoTTE)
        \item Cross-domain evaluation for generalization assessment
        \item Multi-language reranking performance analysis
    \end{itemize}
\end{enumerate}

\subsection{Advanced Research Directions}

\begin{itemize}
    \item \textbf{Theoretical Analysis}: Mathematical characterization of Lion's convergence properties in ranking optimization contexts
    
    \item \textbf{Memory and Efficiency Studies}: Detailed analysis of training time, memory usage, and computational efficiency at scale
    
    \item \textbf{Hybrid Optimization Strategies}: Investigation of combining multiple optimizers during different training phases
    
    \item \textbf{Auto-ML Integration}: Development of automated optimizer selection based on architecture and dataset characteristics
\end{itemize}

\subsection{Practical Applications}

Future work should explore:
\begin{itemize}
    \item \textbf{Production Deployment}: Large-scale evaluation in real-world search systems
    \item \textbf{Specialized Domains}: Medical, legal, and scientific document reranking
    \item \textbf{Multi-Modal Extensions}: Optimization strategies for vision-language retrieval tasks
\end{itemize}

\section{Practical Recommendations}

For practitioners working with cross-encoder reranking:

\begin{itemize}
    \item \textbf{Start with AdamW}: Use AdamW as a robust baseline for initial experiments
    \item \textbf{Consider Lion for Modern Architectures}: Explore Lion optimizer with appropriate learning rate tuning for recent transformer variants
    \item \textbf{Monitor Training Dynamics}: Track multiple metrics and loss curves for optimal checkpoint selection
    \item \textbf{Architecture-Specific Tuning}: Adjust optimizer parameters based on model architecture characteristics
\end{itemize}

\section{Conclusion}

This thesis demonstrates that optimizer choice significantly impacts cross-encoder reranking performance, with the effectiveness varying by architecture and configuration. The Lion optimizer shows particular promise for modern transformer architectures when properly tuned, while AdamW remains a reliable choice across different settings. These findings provide both practical guidance for practitioners and a foundation for future research in neural information retrieval optimization.

\section*{Acknowledgments}

We gratefully acknowledge Modal Labs for providing the cloud computing infrastructure and GPU resources (NVIDIA A100-80GB) that enabled this comprehensive experimental study.