% Chapter Template

\chapter{Summary \& Future Scope of Work}
\label{Chapter6}

This chapter presents a comprehensive summary of our research findings on optimizer effectiveness in cross-encoder reranking and outlines promising directions for future investigation. We reflect on the key insights gained from our experimental analysis and discuss potential avenues for extending this work.

\section{Summary of Research}

Our investigation into the comparative effectiveness of Lion and AdamW optimizers for cross-encoder reranking has yielded several significant findings:

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{Optimizer-Model Interactions:}
    \begin{itemize}
        \item The effectiveness of optimizers showed strong dependence on model architecture and training configuration
        \item ModernBERT achieved optimal performance with Lion optimizer using specialized learning rate settings
        \item GTE demonstrated superior performance with AdamW under standard training parameters
        \item MiniLM showed varying preferences between optimizers depending on the evaluation metric
    \end{itemize}

    \item \textbf{Performance Achievements:}
    \begin{itemize}
        \item ModernBERT with Lion achieved best-in-class results:
        \begin{itemize}
            \item NDCG@10: 0.7225
            \item MAP: 0.5115
            \item R-Precision: 0.5183
        \end{itemize}
        \item Both MiniLM and ModernBERT with Lion achieved state-of-the-art MRR@10 (0.5988) on MS MARCO dev
    \end{itemize}

    \item \textbf{Training Dynamics:}
    \begin{itemize}
        \item Lion demonstrated more stable evaluation loss curves
        \item Cosine Annealing learning rate schedule proved particularly effective with Lion
        \item Gradient behavior showed distinct patterns between optimizers
    \end{itemize}
\end{enumerate}

\subsection{Technical Insights}

The research revealed several important technical considerations:

\begin{itemize}
    \item \textbf{Learning Rate Sensitivity:} The dramatic impact of learning rate selection on Lion's performance suggests careful tuning is essential
    \item \textbf{Context Length Benefits:} Models supporting longer contexts (8192 tokens) showed generally superior performance
    \item \textbf{Architecture Synergies:} Modern architectural features (RoPE, Flash Attention) appeared to complement Lion's optimization characteristics
\end{itemize}

\section{Future Scope of Work}

Our findings open several promising avenues for future research:

\subsection{Technical Extensions}

\begin{enumerate}
    \item \textbf{Hyperparameter Optimization:}
    \begin{itemize}
        \item Comprehensive grid search for Lion's optimal parameters across different model sizes
        \item Investigation of alternative learning rate schedules
        \item Exploration of momentum parameter impacts
    \end{itemize}

    \item \textbf{Architecture Studies:}
    \begin{itemize}
        \item Evaluation of Lion's effectiveness on emerging transformer variants
        \item Investigation of optimization patterns in multi-query attention mechanisms
        \item Analysis of position embedding schemes' interaction with different optimizers
    \end{itemize}

    \item \textbf{Scaling Studies:}
    \begin{itemize}
        \item Analysis of optimizer behavior with larger model sizes
        \item Investigation of training stability at different batch sizes
        \item Evaluation of memory efficiency at scale
    \end{itemize}
\end{enumerate}

\subsection{Application Extensions}

Several practical applications deserve further investigation:

\begin{itemize}
    \item \textbf{Document-Level Reranking:} Leveraging the 8K context capability for full document reranking
    \item \textbf{Multi-Stage Ranking:} Investigating optimizer impact in cascade ranking architectures
    \item \textbf{Cross-Lingual Applications:} Extending the analysis to multilingual reranking scenarios
    \item \textbf{Domain Adaptation:} Studying optimizer effectiveness in domain transfer settings
\end{itemize}

\subsection{Theoretical Investigations}

Future work should also address theoretical aspects:

\begin{itemize}
    \item Analysis of Lion's convergence properties in ranking optimization
    \item Mathematical characterization of optimizer-architecture interactions
    \item Theoretical bounds on performance with different optimizers
\end{itemize}

\section{Recommendations for Practitioners}

Based on our findings, we recommend:

\begin{itemize}
    \item Consider Lion optimizer for modern architectures with appropriate learning rate scheduling
    \item Maintain AdamW as a robust baseline, especially for established architectures
    \item Carefully tune learning rates when using Lion optimizer
    \item Monitor training dynamics through multiple metrics for optimal checkpoint selection
\end{itemize}

\section{Concluding Remarks}

This research has demonstrated the potential of the Lion optimizer in cross-encoder reranking while highlighting the complexity of optimizer-model interactions. The findings provide a foundation for both practical applications and future research directions in neural information retrieval.

\section*{Acknowledgments}

We gratefully acknowledge Modal Labs (\url{https://modal.com/}) for providing the cloud computing infrastructure and GPU resources (NVIDIA A100-80GB) that made this research possible. Their platform enabled efficient experimentation with large-scale models and datasets, contributing significantly to the comprehensive nature of our analysis.